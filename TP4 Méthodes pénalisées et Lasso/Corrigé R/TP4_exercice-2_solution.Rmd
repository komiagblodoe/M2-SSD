---
title: "TP4 - exercice 2 : compromis performance / sparsité - pénalités Lasso, Ridge et Elastic-Net"
output: 
    html_document:
    number_sections: true
    fig_width: 4
    fig_height: 4
documentclass: article
author:  Master parcours SSD - UE Fouille de Données
date: Hiver 2019
---


Dans cet exercice nous allons illustrer le compromis entre performances de prédiction et interprétabilité du modèle que l'on observe fréquemment quand on applique les pénalités Lasso et Ridge à des données de haute dimension (et corrélées) : le Lasso permettant d'obtenir un modèle interprétable, mais la pénalité ridge offrant souvent de meilleures performances de prédiction. Nous illustrerons également que la pénalité elastic-net peut s'avérer intéressante dans ce contexte.

Pour cela nous travaillerons sur le jeu de données **Golub**, un jeu de données fondateur pour l'analyse de données de puces à ADN pour l'oncologie, dont on trouve un descriptif [ici](https://web.stanford.edu/~hastie/CASI_files/DATA/leukemia.html). 

```{r, echo = FALSE}
# define global chunks options
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
# fix random seed
set.seed(20)
```


# Question 1. Charger le jeu de données
Le jeu de données est stocké dans deux fichiers texte **X.txt** et **y.txt** contenant respectivement les observations (72 échantillons en dimension 3571) et les réponses associées (codées ici en -1/1).

```{r}
y = factor(read.table("datasets/golub/y.txt")$V1)
cat("categories counts : " , table(y), "\n")

X = read.table("datasets/golub/X.txt")
X = as.matrix(X)
cat("feature matrix dimensions : ", dim(X), "\n")
```


# Question 2. Visualiser les chemins de régularisation des modèles lasso et ridge

```{r, fig.height = 5, fig.width = 10}
# load package
library(glmnet)
# fit models
fit.lasso = glmnet(x = X, y = y, family = 'binomial')
fit.ridge = glmnet(x = X, y = y, family = 'binomial', alpha = 0)
# plot models
par(mfrow = c(1,2))
plot(fit.lasso, main = "regularization path - lasso")
plot(fit.ridge, main = "regularization path - ridge")
```


On note, comme attendu, que le nombre de variables sélectionnées tout au long du chemin de régularisation du modèle ridge (indiqués au dessus de la figure) est toujours égal au nombre total de variables (3571 ici).
A l'inverse, le lasso sélectionne jusqu'à à peine plus de 20 variables pour la grille de valeurs du paramètre de régularisation considérée.


# Question 3. Réaliser une expérience de validation croisée en utilisant la fonction cv.glmnet() et afficher les résultats avec la fonction plot.cv.glmnet(). A quoi correspondent les deux droites verticales représentées en pointillés ?

On utilisera l'option $\texttt{type.measure = 'class'}$ pour considérer l'erreur de classification comme critère de performance (se référer à la documentation pour davantage de détails).

```{r, fig.height = 5, fig.width = 10}
# load package
library(glmnet)
# fit models
cv.lasso = cv.glmnet(x = X, y = y, family = 'binomial', type.measure = "class")
cv.ridge = cv.glmnet(x = X, y = y, family = 'binomial', alpha = 0, type.measure = "class")
# plot models
par(mfrow = c(1,2))
plot(cv.lasso, main = "cross-validation results - lasso")
plot(cv.ridge, main = "cross-validation results - ridge")
```

Les deux droites verticales représentées en pointillées représentent les modèles proposés automatiquement par la fonction $\texttt{cv.glmnet$, défini par les valeurs $\texttt{lambda.min}$ et $\texttt{lambda.1se}$ du paramètre de régularisation : 

* $\texttt{lambda.min}$ est la valeur permettant d'obtenir la plus faible erreur de classification, en moyenne sur les folds de cross-validation. Avec le lasso, elle conduit ici à sélectionner 23 variables sur les 3571.

* $\texttt{lambda.1se}$ est la valeur la plus élevée (i.e., le modèle le plus parcimonieux) permettant de se rapprocher de ces performances optimales à une erreur-type près. En d'autres termes, la valeur de $\lambda$ la plus élevée dont la performance moyenne (le point rouge) tombe dans l'intervalle défini comme la performance moyenne associée à $\texttt{lambda.min}$ (son point rouge) plus ou moins 1 écart type (mesurant la variabilité obtenue dans les folds de validation croisée). 
\end{itemize}


# Question 4. Afin de mieux interpréter les résultats, représenter sur une même figure l'évolution des performances de validation croisée obtenue par les deux modèles quand le paramètre de régularisation varie. Quel modèle offre les meilleures performances ?

```{r, fig.height = 5}
# extract average performance
perf.lasso = cv.lasso$cvm
perf.ridge = cv.ridge$cvm
n.lambda = length(perf.lasso)
# plot
plot(seq(length(perf.lasso)), perf.lasso, ylim = range(c(perf.lasso,perf.ridge)), type = "l", xlab = "lambda index", ylab = "classification error", main = "cross-validation performance - lasso vs ridge")
lines(seq(length(perf.ridge)), perf.ridge, type = "l", col = 2)
grid()
legend("topright", c("lasso","ridge"), col = c(1,2), lwd = 1)
```

On note que le modèle ridge permet d'obtenir sensiblement de meilleures performances.


# Question 5. Reproduire cette analyse en considérant une penalité elastic-net afin de considérer un compromis entre lasso et ridge. Commenter les résultats.

Il suffit pour cela de modifier le paramètre $\alpha$ de la fonction $\texttt{cv.glmnet()}$ et de le faire varier entre 0 (ridge) et 1 (Lasso, la valeur par défaut).
On pourra par exemple considérer une grille définie par pas de 0.2.


```{r}
# compute enet solutions
alpha.grid = seq(0, 1, by = 0.2)
cv.enet = list()
for(alpha in alpha.grid){
    cv.enet[[paste0("alpha-", alpha)]] = cv.glmnet(x = X, y = y, family = "binomial", type.measure = "class", alpha = alpha)
}
```

```{r, fig.height = 5}
# extract average performance
perf.enet = lapply(cv.enet, function(x){x$cvm})
# plot
ylim = range(unlist(perf.enet))

plot(seq(length(perf.enet[[1]])), perf.enet[[1]], ylim = ylim, type = "l", xlab = "lambda index", ylab = "classification error", main = "cross-validation performance - enet")
grid()
for(i in 2:length(alpha.grid)){
  lines(seq(length(perf.enet[[i]])), perf.enet[[i]], type = "l", col = i)
}

legend("topright", paste("alpha =", alpha.grid), col = seq(length(alpha.grid)), lwd = 1, bg = "white")
```

On constate qu'on peut obtenir des performances équivalentes à la performance ridge en considérant la pénalité elasticnet au lieu du lasso.


# Question 6. Enfin, dresser un bilan des résultats obtenus en représentant l'évolution (1) des meilleures performances de validation croisée et (2) du nombre de variables sélectionnées, en fonction de $\alpha$ . Commenter les résultats. Quelle valeur de  $\alpha$  retiendriez-vous ? Comparer le chemin de régularisation correspondant à celui du lasso.


Pour chaque modèle on retiendra la valeur $\texttt{lambda_1se}$  proposée par la fonction $\texttt{cv.glmnet()}$ comme "meilleur" paramètre de régularisation.

Notons que le champ $\texttt{nzero}$  de l'objet renvoyé par $\texttt{cv.glmnet()}$ donne le nombre de coefficients non-nuls pour chaque valeur de  $\lambda$ considérée (l'ensemble de ces valeurs étant stockées dans le champ  $\texttt{lambda}$).


```{r, fig.height = 5, fig.width = 10}
# extract support
support = sapply(cv.enet, function(x){x$nzero[which(x$lambda == x$lambda.1se)]})
# extract performance
perf = sapply(cv.enet, function(x){x$cvm[which(x$lambda == x$lambda.1se)]})
# plot
par(mfrow = c(1,2))
barplot(support, names = alpha.grid, xlab = "alpha", ylab = "number of active variables", ylim = c(0,100), main = "support size vs alpha", xpd = F)
barplot(perf, names = alpha.grid, xlab = "alpha", ylab = "number of active variables",  main = "cross-validation error vs alpha")
```


On constate qu'avec un nombre de variables sensiblement plus important qu'avec le lasso on peut obtenir des performances du même ordre qu'avec le modèle ridge (qui lui met en jeu les 3571 variables du jeu de données).


Au vu de ces résultats, une valeur de $\alpha$ égale à 0.4 semble adaptée, permettant d'avoir un bon compromis entre performance et parcimonie.
La figure suivante compare les chemins de régularisation correspondant.


```{r, fig.height = 5, fig.width = 10}
# fit models
fit.lasso = glmnet(x = X, y = y, family = 'binomial')
fit.enet = glmnet(x = X, y = y, family = 'binomial', alpha = 0.4)
# plot models
par(mfrow = c(1,2))
plot(fit.lasso, main = "regularization path - lasso")
plot(fit.enet, main = "regularization path - elastic-net")
```
